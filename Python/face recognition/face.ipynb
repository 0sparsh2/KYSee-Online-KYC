{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8480a36e6250e9d9603b20b58071481133dd4dd026948e2d287c77aeff746014"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 8s 0us/step\n",
      "0 InputLayer True\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows, img_cols = 224, 224 \n",
    "\n",
    "# Loads the VGG16 model without the top or FC layers\n",
    "model = vgg16.VGG16(weights='imagenet', include_top = False, input_shape=(img_rows,img_cols,3))\n",
    "\n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    #creates the top or head of the model that will be placed ontop of the bottom layers\n",
    "\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               6422784   \n_________________________________________________________________\ndropout (Dropout)            (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 21,137,986\nTrainable params: 6,423,298\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = 2\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs = model.input, outputs = FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 218 images belonging to 2 classes.\nFound 218 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "path = '/Users/Anil/python codes/coduh2/myself'\n",
    "train_data_dir =path\n",
    "\n",
    "validation_data_dir = path\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=45,\n",
    "      width_shift_range=0.3,\n",
    "      height_shift_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Set the Batch Size according to your system.\n",
    "train_batchsize = 10\n",
    "val_batchsize = 5\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 61s 6s/step - loss: 7.9444 - accuracy: 0.5343 - val_loss: 0.3015 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 60s 6s/step - loss: 0.6773 - accuracy: 0.6318 - val_loss: 0.3307 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 59s 6s/step - loss: 0.4333 - accuracy: 0.8265 - val_loss: 0.1338 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 54s 6s/step - loss: 0.4028 - accuracy: 0.8341 - val_loss: 0.1124 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 55s 6s/step - loss: 0.4186 - accuracy: 0.8358 - val_loss: 0.2031 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"facedetect.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# We put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "# We use a very small learning rate \n",
    "modelnew.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Enter the number of training and validation samples here\n",
    "nb_train_samples = 100\n",
    "nb_validation_samples = 100\n",
    "\n",
    "# We only train 5 EPOCHS \n",
    "epochs = 5\n",
    "batch_size = 5\n",
    "\n",
    "history = modelnew.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "modelnew.save('facedetect.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "m = load_model(\"facedetect.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "[[0. 1.]]\n",
      "try again\n"
     ]
    }
   ],
   "source": [
    "#Path to my sample pic\n",
    "filename = '/Users/Anil/Desktop/trial.jpg'# + '.jpg'\n",
    "\n",
    "cimage = image.load_img(filename, target_size=(224,224,3))\n",
    "print(cimage.show())\n",
    "\n",
    "cimage = image.img_to_array(cimage)\n",
    "cimage = np.expand_dims(cimage, axis=0)\n",
    "result = m.predict(cimage)\n",
    "print(result)\n",
    "if(result[0][0]>result[0][1]):\n",
    "    print(\"Verified\")\n",
    "else:\n",
    "    print(\"try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread('/Users/Anil/Desktop/trial.jpg')   \n",
    "cv2.imshow(\"\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-48c801ya/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cf3475ced321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/Anil/Desktop/trial.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-48c801ya/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite('C:/Users/Anil/Desktop/trial2.jpg', image_gray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}